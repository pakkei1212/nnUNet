{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb5e05e",
   "metadata": {},
   "source": [
    "# nnU-Net Pipeline Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a1b8a",
   "metadata": {},
   "source": [
    "This notebook replicates the end-to-end nnU-Net pipeline (data preparation, training, inference) in three separate stages.\n",
    "Update the configuration cell, then execute the subsequent sections sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af11878",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e5c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe433ef",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Edit the fields of `PipelineConfig` or override `cfg` attributes after instantiation to match your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8815e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    data_root: Path = field(default_factory=lambda: Path(\"public_leaderboard_data\"))\n",
    "    dataset_id: int = 500\n",
    "    dataset_name: str = \"AbdominalCTMultiOrgan\"\n",
    "    nnunet_raw: Path = field(default_factory=lambda: Path(\"./nnUNet_raw\"))\n",
    "    nnunet_preprocessed: Path = field(default_factory=lambda: Path(\"./nnUNet_preprocessed\"))\n",
    "    nnunet_results: Path = field(default_factory=lambda: Path(\"./nnUNet_results\"))\n",
    "    configurations: Sequence[str] = (\"3d_fullres\",)\n",
    "    trainer_class: str = \"nnUNetTrainer\"\n",
    "    plans_identifier: str = \"nnUNetResEncUNetMPlans\"\n",
    "    fold: str = \"all\"\n",
    "    device: str = \"cuda\"\n",
    "    num_gpus: int = 1\n",
    "    num_processes_fingerprint: int = 4\n",
    "    num_processes_preprocess: int = 4\n",
    "    prediction_output: Optional[Path] = None\n",
    "    checkpoint_name: str = \"checkpoint_best.pth\"\n",
    "    planner_class: str = \"nnUNetPlannerResEncM\"\n",
    "    gpu_memory_target: Optional[float] = None\n",
    "    preprocessor_class: str = \"DefaultPreprocessor\"\n",
    "    verify_dataset: bool = False\n",
    "    skip_conversion: bool = False\n",
    "    skip_preprocessing: bool = False\n",
    "    skip_training: bool = False\n",
    "    skip_validation_inference: bool = False\n",
    "    skip_test_inference: bool = False\n",
    "    export_test_pngs: bool = True\n",
    "    png_output_root: Optional[Path] = None\n",
    "    overwrite: bool = False\n",
    "    save_probabilities: bool = False\n",
    "    export_validation_probabilities: bool = False\n",
    "    inference_preprocess_workers: int = 1\n",
    "    inference_export_workers: int = 0  # 0 runs export sequentially to keep RAM usage low\n",
    "    bounding_box_prompts: Optional[Path] = None\n",
    "    only_configuration: Optional[str] = None\n",
    "    log_to_stdout: bool = True\n",
    "\n",
    "    def clone(self) -> \"PipelineConfig\":\n",
    "        return PipelineConfig(**self.__dict__)\n",
    "\n",
    "\n",
    "cfg = PipelineConfig()\n",
    "\n",
    "if cfg.prediction_output is None:\n",
    "    cfg.prediction_output = Path(\"./notebook_predictions\")\n",
    "else:\n",
    "    cfg.prediction_output = Path(cfg.prediction_output)\n",
    "cfg.prediction_output.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cf5e1",
   "metadata": {},
   "source": [
    "## Utility Functions & Dataset Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb246063",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nnunetv2.dataset_conversion.generate_dataset_json import generate_dataset_json\n",
    "\n",
    "def configure_environment(cfg: PipelineConfig) -> None:\n",
    "    os.environ.setdefault(\"nnUNet_raw\", str(cfg.nnunet_raw.resolve()))\n",
    "    os.environ.setdefault(\"nnUNet_preprocessed\", str(cfg.nnunet_preprocessed.resolve()))\n",
    "    os.environ.setdefault(\"nnUNet_results\", str(cfg.nnunet_results.resolve()))\n",
    "    for root in (cfg.nnunet_raw, cfg.nnunet_preprocessed, cfg.nnunet_results):\n",
    "        Path(root).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def ensure_dependencies() -> None:\n",
    "    try:\n",
    "        import torch  # noqa: F401\n",
    "    except ImportError as exc:\n",
    "        raise RuntimeError(\"PyTorch is required to run the pipeline. Install project dependencies first.\") from exc\n",
    "\n",
    "\n",
    "def parse_spacing_map(spacing_file: Path) -> Dict[str, Tuple[float, float, float]]:\n",
    "    if not spacing_file.exists():\n",
    "        raise FileNotFoundError(f\"Spacing file not found: {spacing_file}\")\n",
    "    mapping: Dict[str, Tuple[float, float, float]] = {}\n",
    "    with spacing_file.open(\"r\") as f:\n",
    "        for raw_line in f:\n",
    "            line = raw_line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            key, value = line.split(\":\", 1)\n",
    "            case_id = key.strip().zfill(2)\n",
    "            spacing = eval(value.strip(), {\"__builtins__\": {}})\n",
    "            if not isinstance(spacing, (list, tuple)) or len(spacing) != 3:\n",
    "                raise ValueError(f\"Unexpected spacing entry for case {case_id}: {value}\")\n",
    "            mapping[case_id] = tuple(float(v) for v in spacing)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def sorted_slice_paths(case_folder: Path) -> List[Path]:\n",
    "    slices = sorted(case_folder.glob(\"*.png\"), key=lambda p: int(p.stem))\n",
    "    if not slices:\n",
    "        raise FileNotFoundError(f\"No PNG slices found in {case_folder}\")\n",
    "    return slices\n",
    "\n",
    "\n",
    "def load_stack(slice_paths: Sequence[Path]) -> np.ndarray:\n",
    "    stack = [io.imread(str(p)) for p in slice_paths]\n",
    "    return np.stack(stack, axis=0)\n",
    "\n",
    "\n",
    "def write_nifti(volume: np.ndarray, spacing: Tuple[float, float, float], output_path: Path, dtype: np.dtype) -> None:\n",
    "    img = sitk.GetImageFromArray(volume.astype(dtype, copy=False))\n",
    "    img.SetSpacing(tuple(float(v) for v in spacing))\n",
    "    img.SetOrigin((0.0, 0.0, 0.0))\n",
    "    img.SetDirection((1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0))\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sitk.WriteImage(img, str(output_path))\n",
    "\n",
    "\n",
    "def convert_split_to_nnunet(\n",
    "    case_ids: Iterable[str],\n",
    "    image_root: Path,\n",
    "    label_root: Optional[Path],\n",
    "    output_images: Path,\n",
    "    output_labels: Optional[Path],\n",
    "    spacing_map: Dict[str, Tuple[float, float, float]],\n",
    "    prefix: str,\n",
    "    overwrite: bool,\n",
    ") -> Tuple[List[str], Dict[str, str]]:\n",
    "    case_identifiers: List[str] = []\n",
    "    case_mapping: Dict[str, str] = {}\n",
    "    for case_id in sorted(case_ids, key=lambda x: int(x)):\n",
    "        image_case_dir = image_root / case_id\n",
    "        label_case_dir = label_root / case_id if label_root is not None else None\n",
    "        if not image_case_dir.is_dir():\n",
    "            raise FileNotFoundError(f\"Missing image folder for case {case_id}: {image_case_dir}\")\n",
    "        spacing = spacing_map.get(case_id)\n",
    "        if spacing is None:\n",
    "            raise KeyError(f\"No spacing metadata for case {case_id} in spacing file\")\n",
    "        case_name = f\"{prefix}_{case_id.zfill(3)}\"\n",
    "        image_output_path = output_images / f\"{case_name}_0000.nii.gz\"\n",
    "        if image_output_path.exists() and not overwrite:\n",
    "            case_identifiers.append(case_name)\n",
    "            case_mapping[case_name] = str(case_id).zfill(2)\n",
    "            continue\n",
    "        slices = sorted_slice_paths(image_case_dir)\n",
    "        volume = load_stack(slices).astype(np.int16, copy=False)\n",
    "        write_nifti(volume, spacing, image_output_path, np.int16)\n",
    "        if label_case_dir is not None:\n",
    "            if output_labels is None:\n",
    "                raise ValueError(\"Label root provided but output label directory missing.\")\n",
    "            label_slices = sorted_slice_paths(label_case_dir)\n",
    "            if len(label_slices) != len(slices):\n",
    "                raise ValueError(\n",
    "                    f\"Mismatched slice count for case {case_id}: {len(slices)} images vs {len(label_slices)} labels\"\n",
    "                )\n",
    "            label_volume = load_stack(label_slices).astype(np.uint8, copy=False)\n",
    "            label_output_path = output_labels / f\"{case_name}.nii.gz\"\n",
    "            write_nifti(label_volume, spacing, label_output_path, np.uint8)\n",
    "        case_identifiers.append(case_name)\n",
    "        case_mapping[case_name] = str(case_id).zfill(2)\n",
    "    return case_identifiers, case_mapping\n",
    "\n",
    "\n",
    "def parse_bbox_prompts(bbox_file: Optional[Path], output_json: Path) -> None:\n",
    "    if not bbox_file or not bbox_file.exists():\n",
    "        return\n",
    "    prompts: Dict[str, Dict[str, Dict[str, Sequence[int]]]] = {}\n",
    "    with bbox_file.open(\"r\") as f:\n",
    "        for raw_line in f:\n",
    "            line = raw_line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if not line.startswith(\"<\") or \">:\" not in line:\n",
    "                continue\n",
    "            key, value = line.split(\">:\")\n",
    "            triplet = key.strip(\"<>\").split(\",\")\n",
    "            if len(triplet) != 3:\n",
    "                continue\n",
    "            case_id = triplet[0].strip().zfill(2)\n",
    "            slice_idx = triplet[1].strip()\n",
    "            organ_idx = triplet[2].strip()\n",
    "            coords = eval(value.strip(), {\"__builtins__\": {}})\n",
    "            prompts.setdefault(case_id, {}).setdefault(slice_idx, {})[organ_idx] = coords\n",
    "    output_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_json.open(\"w\") as f:\n",
    "        json.dump(prompts, f, indent=2)\n",
    "\n",
    "\n",
    "def generate_dataset_json_file(\n",
    "    dataset_dir: Path,\n",
    "    num_training_cases: int,\n",
    "    labels: Dict[str, int],\n",
    "    dataset_name: str,\n",
    "    metadata: Dict[str, object],\n",
    ") -> None:\n",
    "    generate_dataset_json(\n",
    "        str(dataset_dir),\n",
    "        channel_names={0: \"CT\"},\n",
    "        labels=labels,\n",
    "        num_training_cases=num_training_cases,\n",
    "        file_ending=\".nii.gz\",\n",
    "        dataset_name=dataset_name,\n",
    "        **metadata,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_raw_dataset(cfg: PipelineConfig, dataset_dir: Path) -> Tuple[Dict[str, List[str]], Dict[str, str]]:\n",
    "    spacing_map = parse_spacing_map(cfg.data_root / \"spacing_mm.txt\")\n",
    "    train_ids = [p.name for p in (cfg.data_root / \"train_images\").iterdir() if p.is_dir()]\n",
    "    val_ids = [p.name for p in (cfg.data_root / \"val_images\").iterdir() if p.is_dir()]\n",
    "    test_ids = [p.name for p in (cfg.data_root / \"test1_images\").iterdir() if p.is_dir()]\n",
    "\n",
    "    images_tr = dataset_dir / \"imagesTr\"\n",
    "    labels_tr = dataset_dir / \"labelsTr\"\n",
    "    images_ts = dataset_dir / \"imagesTs\"\n",
    "    images_tr.mkdir(parents=True, exist_ok=True)\n",
    "    labels_tr.mkdir(parents=True, exist_ok=True)\n",
    "    images_ts.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_cases, train_map = convert_split_to_nnunet(\n",
    "        train_ids,\n",
    "        cfg.data_root / \"train_images\",\n",
    "        cfg.data_root / \"train_labels\",\n",
    "        images_tr,\n",
    "        labels_tr,\n",
    "        spacing_map,\n",
    "        prefix=\"ct\",\n",
    "        overwrite=cfg.overwrite,\n",
    "    )\n",
    "    val_cases, val_map = convert_split_to_nnunet(\n",
    "        val_ids,\n",
    "        cfg.data_root / \"val_images\",\n",
    "        cfg.data_root / \"val_labels\",\n",
    "        images_tr,\n",
    "        labels_tr,\n",
    "        spacing_map,\n",
    "        prefix=\"ct\",\n",
    "        overwrite=cfg.overwrite,\n",
    "    )\n",
    "    test_cases, test_map = convert_split_to_nnunet(\n",
    "        test_ids,\n",
    "        cfg.data_root / \"test1_images\",\n",
    "        None,\n",
    "        images_ts,\n",
    "        None,\n",
    "        spacing_map,\n",
    "        prefix=\"ct\",\n",
    "        overwrite=cfg.overwrite,\n",
    "    )\n",
    "\n",
    "    metadata = {\n",
    "        \"training_cases\": train_cases,\n",
    "        \"validation_cases\": val_cases,\n",
    "        \"test_cases\": test_cases,\n",
    "        \"spacing_file\": str((cfg.data_root / \"spacing_mm.txt\").resolve()),\n",
    "        \"case_folder_map\": {**train_map, **val_map, **test_map},\n",
    "    }\n",
    "\n",
    "    labels = {\"background\": 0}\n",
    "    for organ_idx in range(1, 13):\n",
    "        labels[f\"organ_{organ_idx:02d}\"] = organ_idx\n",
    "\n",
    "    generate_dataset_json_file(\n",
    "        dataset_dir=dataset_dir,\n",
    "        num_training_cases=len(train_cases) + len(val_cases),\n",
    "        labels=labels,\n",
    "        dataset_name=dataset_dir.name,\n",
    "        metadata=metadata,\n",
    "    )\n",
    "\n",
    "    splits = [{\"train\": train_cases, \"val\": val_cases}]\n",
    "    splits_file = dataset_dir / \"splits_final.json\"\n",
    "    if not splits_file.exists() or cfg.overwrite:\n",
    "        with splits_file.open(\"w\") as f:\n",
    "            json.dump(splits, f, indent=2)\n",
    "\n",
    "    if cfg.bounding_box_prompts:\n",
    "        parse_bbox_prompts(cfg.bounding_box_prompts, dataset_dir / \"test_bboxes.json\")\n",
    "\n",
    "    return {\"train\": train_cases, \"val\": val_cases, \"test\": test_cases}, {**train_map, **val_map, **test_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc9abe",
   "metadata": {},
   "source": [
    "## Training & Inference Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0d6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_planning_and_preprocessing(cfg: PipelineConfig, configurations: Sequence[str]) -> str:\n",
    "    from nnunetv2.experiment_planning.plan_and_preprocess_api import (\n",
    "        extract_fingerprints,\n",
    "        plan_experiments,\n",
    "        preprocess,\n",
    "    )\n",
    "\n",
    "    dataset_ids = [cfg.dataset_id]\n",
    "    extract_fingerprints(\n",
    "        dataset_ids,\n",
    "        num_processes=cfg.num_processes_fingerprint,\n",
    "        check_dataset_integrity=cfg.verify_dataset,\n",
    "        clean=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    resulting = plan_experiments(\n",
    "        dataset_ids,\n",
    "        experiment_planner_class_name=cfg.planner_class,\n",
    "        preprocess_class_name=cfg.preprocessor_class,\n",
    "        gpu_memory_target_in_gb=cfg.gpu_memory_target,\n",
    "    )\n",
    "    preprocess(\n",
    "        dataset_ids,\n",
    "        plans_identifier=resulting or cfg.plans_identifier,\n",
    "        configurations=tuple(configurations),\n",
    "        num_processes=tuple([cfg.num_processes_preprocess] * len(configurations)),\n",
    "        verbose=True,\n",
    "    )\n",
    "    return resulting or cfg.plans_identifier\n",
    "\n",
    "\n",
    "def build_model_output_dir(dataset_name: str, trainer_class: str, plans_identifier: str, configuration: str) -> Path:\n",
    "    base = Path(os.environ[\"nnUNet_results\"])\n",
    "    return base / dataset_name / f\"{trainer_class}__{plans_identifier}__{configuration}\"\n",
    "\n",
    "\n",
    "def run_training_stage(\n",
    "    cfg: PipelineConfig,\n",
    "    dataset_name: str,\n",
    "    configuration: str,\n",
    "    plans_identifier: str,\n",
    ") -> None:\n",
    "    import torch\n",
    "    from nnunetv2.run.run_training import run_training\n",
    "\n",
    "    torch_device = torch.device(cfg.device)\n",
    "    run_training(\n",
    "        dataset_name,\n",
    "        configuration=configuration,\n",
    "        fold=cfg.fold,\n",
    "        trainer_class_name=cfg.trainer_class,\n",
    "        plans_identifier=plans_identifier,\n",
    "        num_gpus=cfg.num_gpus,\n",
    "        device=torch_device,\n",
    "        export_validation_probabilities=cfg.export_validation_probabilities,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_inference(\n",
    "    model_dir: Path,\n",
    "    fold: str,\n",
    "    inputs: Sequence[List[str]],\n",
    "    output_dir: Path,\n",
    "    device: str,\n",
    "    checkpoint_name: str,\n",
    "    save_probabilities: bool,\n",
    "    overwrite: bool,\n",
    "    num_preprocess_workers: int,\n",
    "    num_export_workers: int,\n",
    ") -> None:\n",
    "    import gc\n",
    "    import torch\n",
    "    from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "\n",
    "    predictor = nnUNetPredictor(\n",
    "        tile_step_size=0.5,\n",
    "        use_gaussian=True,\n",
    "        use_mirroring=False,\n",
    "        perform_everything_on_device=True,\n",
    "        device=torch.device(device),\n",
    "        verbose=False,\n",
    "        verbose_preprocessing=False,\n",
    "        allow_tqdm=True\n",
    "    )\n",
    "    predictor.initialize_from_trained_model_folder(str(model_dir), use_folds=(fold,), checkpoint_name=checkpoint_name)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    preprocess_workers = max(1, num_preprocess_workers)\n",
    "    export_workers = max(0, num_export_workers)\n",
    "    input_batches = list(inputs)\n",
    "\n",
    "    if export_workers == 0:\n",
    "        predictor.predict_from_files_sequential(\n",
    "            input_batches,\n",
    "            str(output_dir),\n",
    "            save_probabilities=save_probabilities,\n",
    "            overwrite=overwrite,\n",
    "        )\n",
    "    else:\n",
    "        predictor.predict_from_files(\n",
    "            input_batches,\n",
    "            str(output_dir),\n",
    "            save_probabilities=save_probabilities,\n",
    "            overwrite=overwrite,\n",
    "            num_processes_preprocessing=preprocess_workers,\n",
    "            num_processes_segmentation_export=export_workers,\n",
    "        )\n",
    "\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def compute_validation_metrics(\n",
    "    predictions_dir: Path,\n",
    "    dataset_dir: Path,\n",
    "    plans_identifier: str,\n",
    "    output_filename: Optional[Path] = None,\n",
    ") -> Dict[str, object]:\n",
    "    from nnunetv2.evaluation.evaluate_predictions import compute_metrics_on_folder2\n",
    "\n",
    "    dataset_json = dataset_dir / \"dataset.json\"\n",
    "    plans_file = Path(os.environ[\"nnUNet_preprocessed\"]) / dataset_dir.name / f\"{plans_identifier}.json\"\n",
    "    gt_folder = dataset_dir / \"labelsTr\"\n",
    "    return compute_metrics_on_folder2(\n",
    "        str(gt_folder),\n",
    "        str(predictions_dir),\n",
    "        str(dataset_json),\n",
    "        str(plans_file),\n",
    "        output_file=str(output_filename) if output_filename else None,\n",
    "        chill=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_inference_input_lists(image_dir: Path, case_ids: Sequence[str], file_ending: str) -> List[List[str]]:\n",
    "    inputs: List[List[str]] = []\n",
    "    for case in case_ids:\n",
    "        image_file = image_dir / f\"{case}_0000{file_ending}\"\n",
    "        if not image_file.exists():\n",
    "            raise FileNotFoundError(f\"Missing input volume for inference: {image_file}\")\n",
    "        inputs.append([str(image_file)])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def convert_nifti_to_png_slices(nifti_file: Path, output_dir: Path) -> None:\n",
    "    img = sitk.ReadImage(str(nifti_file))\n",
    "    data = sitk.GetArrayFromImage(img).astype(np.uint8, copy=False)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for idx, slice_arr in enumerate(data, start=1):\n",
    "        slice_path = output_dir / f\"{idx}.png\"\n",
    "        io.imsave(str(slice_path), slice_arr, check_contrast=False)\n",
    "\n",
    "\n",
    "def export_predictions_to_png(\n",
    "    predictions_dir: Path,\n",
    "    output_root: Path,\n",
    "    case_folder_map: Dict[str, str],\n",
    ") -> None:\n",
    "    prediction_files = sorted(predictions_dir.glob(\"*.nii.gz\"))\n",
    "    if not prediction_files:\n",
    "        return\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "    for prediction_file in prediction_files:\n",
    "        filename = prediction_file.name\n",
    "        identifier = filename[:-7] if filename.endswith(\".nii.gz\") else prediction_file.stem\n",
    "        folder_name = case_folder_map.get(identifier, identifier.split(\"_\")[-1])\n",
    "        folder_name = str(int(folder_name)).zfill(2) if folder_name.isdigit() else folder_name\n",
    "        convert_nifti_to_png_slices(prediction_file, output_root / folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f2f9cd",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Run this cell to convert the PNG slices into the nnU-Net raw data structure (unless skipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e42cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted dataset stored at /mnt/c/Projects/nnUNet/nnUNet_raw/Dataset500_AbdominalCTMultiOrgan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': ['ct_001',\n",
       "  'ct_002',\n",
       "  'ct_003',\n",
       "  'ct_004',\n",
       "  'ct_005',\n",
       "  'ct_006',\n",
       "  'ct_007',\n",
       "  'ct_008',\n",
       "  'ct_009',\n",
       "  'ct_010',\n",
       "  'ct_011',\n",
       "  'ct_012',\n",
       "  'ct_013',\n",
       "  'ct_014',\n",
       "  'ct_015',\n",
       "  'ct_016',\n",
       "  'ct_017',\n",
       "  'ct_018',\n",
       "  'ct_019',\n",
       "  'ct_020',\n",
       "  'ct_021',\n",
       "  'ct_022',\n",
       "  'ct_023',\n",
       "  'ct_024',\n",
       "  'ct_025',\n",
       "  'ct_026',\n",
       "  'ct_027',\n",
       "  'ct_028',\n",
       "  'ct_029',\n",
       "  'ct_030',\n",
       "  'ct_031',\n",
       "  'ct_032',\n",
       "  'ct_033',\n",
       "  'ct_034',\n",
       "  'ct_035',\n",
       "  'ct_036',\n",
       "  'ct_037',\n",
       "  'ct_038',\n",
       "  'ct_039',\n",
       "  'ct_040'],\n",
       " 'val': ['ct_041',\n",
       "  'ct_042',\n",
       "  'ct_043',\n",
       "  'ct_044',\n",
       "  'ct_045',\n",
       "  'ct_046',\n",
       "  'ct_047',\n",
       "  'ct_048',\n",
       "  'ct_049',\n",
       "  'ct_050'],\n",
       " 'test': ['ct_051',\n",
       "  'ct_052',\n",
       "  'ct_053',\n",
       "  'ct_054',\n",
       "  'ct_055',\n",
       "  'ct_056',\n",
       "  'ct_057',\n",
       "  'ct_058',\n",
       "  'ct_059',\n",
       "  'ct_060',\n",
       "  'ct_061',\n",
       "  'ct_062',\n",
       "  'ct_063',\n",
       "  'ct_064',\n",
       "  'ct_065']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "configure_environment(cfg)\n",
    "ensure_dependencies()\n",
    "\n",
    "dataset_name = f\"Dataset{cfg.dataset_id:03d}_{cfg.dataset_name}\"\n",
    "dataset_dir = Path(os.environ[\"nnUNet_raw\"]) / dataset_name\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if cfg.skip_conversion and (dataset_dir / \"dataset.json\").exists():\n",
    "    with (dataset_dir / \"dataset.json\").open(\"r\") as f:\n",
    "        dataset_meta = json.load(f)\n",
    "    case_splits = {\n",
    "        \"train\": dataset_meta.get(\"training_cases\", []),\n",
    "        \"val\": dataset_meta.get(\"validation_cases\", []),\n",
    "        \"test\": dataset_meta.get(\"test_cases\", []),\n",
    "    }\n",
    "    raw_map = dataset_meta.get(\"case_folder_map\", {}) or {}\n",
    "    case_folder_map = {k: str(v).zfill(2) for k, v in raw_map.items()}\n",
    "    if cfg.log_to_stdout:\n",
    "        print(\"Skipping dataset conversion (dataset.json already present).\")\n",
    "else:\n",
    "    case_splits, case_folder_map = prepare_raw_dataset(cfg, dataset_dir)\n",
    "    if cfg.log_to_stdout:\n",
    "        print(f\"Converted dataset stored at {dataset_dir}\")\n",
    "\n",
    "if not case_folder_map:\n",
    "    case_folder_map = {identifier: identifier.split(\"_\")[-1] for identifier in case_splits.get(\"test\", [])}\n",
    "\n",
    "active_configurations = list((cfg.only_configuration,) if cfg.only_configuration else cfg.configurations)\n",
    "case_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a1f8f0",
   "metadata": {},
   "source": [
    "## Training\n",
    "Preprocess the dataset and train the requested configurations/folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f6f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset500_AbdominalCTMultiOrgan\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-14:\n",
      "Process SpawnPoolWorker-13:\n",
      "Process SpawnPoolWorker-15:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mskip_preprocessing:\n\u001b[0;32m----> 2\u001b[0m     plans_identifier \u001b[38;5;241m=\u001b[39m \u001b[43mrun_planning_and_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactive_configurations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     plans_identifier \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mplans_identifier\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mrun_planning_and_preprocessing\u001b[0;34m(cfg, configurations)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnunetv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_planning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplan_and_preprocess_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     extract_fingerprints,\n\u001b[1;32m      4\u001b[0m     plan_experiments,\n\u001b[1;32m      5\u001b[0m     preprocess,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m dataset_ids \u001b[38;5;241m=\u001b[39m [cfg\u001b[38;5;241m.\u001b[39mdataset_id]\n\u001b[0;32m----> 9\u001b[0m \u001b[43mextract_fingerprints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_processes_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_dataset_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m resulting \u001b[38;5;241m=\u001b[39m plan_experiments(\n\u001b[1;32m     17\u001b[0m     dataset_ids,\n\u001b[1;32m     18\u001b[0m     experiment_planner_class_name\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mplanner_class,\n\u001b[1;32m     19\u001b[0m     preprocess_class_name\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpreprocessor_class,\n\u001b[1;32m     20\u001b[0m     gpu_memory_target_in_gb\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mgpu_memory_target,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m preprocess(\n\u001b[1;32m     23\u001b[0m     dataset_ids,\n\u001b[1;32m     24\u001b[0m     plans_identifier\u001b[38;5;241m=\u001b[39mresulting \u001b[38;5;129;01mor\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mplans_identifier,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Projects/nnUNet/nnunetv2/experiment_planning/plan_and_preprocess_api.py:47\u001b[0m, in \u001b[0;36mextract_fingerprints\u001b[0;34m(dataset_ids, fingerprint_extractor_class_name, num_processes, check_dataset_integrity, clean, verbose)\u001b[0m\n\u001b[1;32m     43\u001b[0m fingerprint_extractor_class \u001b[38;5;241m=\u001b[39m recursive_find_python_class(join(nnunetv2\u001b[38;5;241m.\u001b[39m__path__[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_planning\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     44\u001b[0m                                                           fingerprint_extractor_class_name,\n\u001b[1;32m     45\u001b[0m                                                           current_module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnnunetv2.experiment_planning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dataset_ids:\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mextract_fingerprint_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfingerprint_extractor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_dataset_integrity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Projects/nnUNet/nnunetv2/experiment_planning/plan_and_preprocess_api.py:33\u001b[0m, in \u001b[0;36mextract_fingerprint_dataset\u001b[0;34m(dataset_id, fingerprint_extractor_class, num_processes, check_dataset_integrity, clean, verbose)\u001b[0m\n\u001b[1;32m     30\u001b[0m     verify_dataset_integrity(join(nnUNet_raw, dataset_name), num_processes)\n\u001b[1;32m     32\u001b[0m fpe \u001b[38;5;241m=\u001b[39m fingerprint_extractor_class(dataset_id, num_processes, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverwrite_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Projects/nnUNet/nnunetv2/experiment_planning/dataset_fingerprint/fingerprint_extractor.py:155\u001b[0m, in \u001b[0;36mDatasetFingerprintExtractor.run\u001b[0;34m(self, overwrite_existing)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 pbar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    154\u001b[0m             remaining \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m remaining \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m done]\n\u001b[0;32m--> 155\u001b[0m             \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# results = ptqdm(DatasetFingerprintExtractor.analyze_case,\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m#                 (training_images_per_case, training_labels_per_case),\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m#                 processes=self.num_processes, zipped=True, reader_writer_class=reader_writer_class,\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m#                 num_samples=num_foreground_samples_per_case, disable=self.verbose)\u001b[39;00m\n\u001b[1;32m    161\u001b[0m results \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mget()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m r]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if not cfg.skip_preprocessing:\n",
    "    plans_identifier = run_planning_and_preprocessing(cfg, active_configurations)\n",
    "else:\n",
    "    plans_identifier = cfg.plans_identifier\n",
    "    if cfg.log_to_stdout:\n",
    "        print(\"Skipping planning & preprocessing.\")\n",
    "\n",
    "model_directories: Dict[str, Path] = {}\n",
    "for configuration in active_configurations:\n",
    "    model_dir = build_model_output_dir(dataset_name, cfg.trainer_class, plans_identifier, configuration)\n",
    "    model_directories[configuration] = model_dir\n",
    "    if cfg.skip_training:\n",
    "        if cfg.log_to_stdout:\n",
    "            print(f\"Skipping training for configuration {configuration}.\")\n",
    "        continue\n",
    "    if cfg.log_to_stdout:\n",
    "        print(f\"Starting training for configuration {configuration} (fold {cfg.fold})...\")\n",
    "    run_training_stage(cfg, dataset_name, configuration, plans_identifier)\n",
    "\n",
    "model_directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a625fda",
   "metadata": {},
   "source": [
    "## Inference & Evaluation\n",
    "Generate validation metrics and export test predictions (optionally as PNG slices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e7746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping planning & preprocessing.\n",
      "There are 10 cases in the source folder\n",
      "I am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 10 cases that I would like to predict\n",
      "overwrite was set to False, so I am only working on cases that haven't been predicted yet. That's 0 cases.\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "There are 15 cases in the source folder\n",
      "I am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 15 cases that I would like to predict\n",
      "overwrite was set to False, so I am only working on cases that haven't been predicted yet. That's 8 cases.\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 150/150 [00:18<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 120/120 [00:13<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 80/80 [00:09<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 80/80 [00:09<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100/100 [00:11<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100/100 [00:11<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 36/36 [00:03<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PNG segmentations saved to public_leaderboard_data/test_labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('all', 'validation'): None, ('all', 'test'): 'notebook_predictions/all/test'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results: Dict[Tuple[str, str], object] = {}\n",
    "configuration='all'\n",
    "file_ending = \".nii.gz\"\n",
    "\n",
    "\n",
    "plans_identifier = cfg.plans_identifier\n",
    "if cfg.log_to_stdout:\n",
    "    print(\"Skipping planning & preprocessing.\")\n",
    "# model_dir=Path(\"nnUNet_results/Dataset500_AbdominalCTMultiOrgan/nnUNetTrainer__nnUNetResEncUNetMPlans__3d_fullres/fold_all\")\n",
    "# for configuration, model_dir in model_directories.items():\n",
    "model_dir = Path(\"./nnUNet_results/Dataset500_AbdominalCTMultiOrgan/nnUNetTrainer__nnUNetResEncUNetMPlans__3d_fullres/\")\n",
    "fold_dir = model_dir / f\"fold_{cfg.fold}\"\n",
    "if not fold_dir.exists():\n",
    "    raise FileNotFoundError(f\"Expected trained fold directory does not exist: {fold_dir}\")\n",
    "\n",
    "if not cfg.skip_validation_inference and case_splits.get(\"val\"):\n",
    "    val_inputs = build_inference_input_lists(dataset_dir / \"imagesTr\", case_splits[\"val\"], file_ending)\n",
    "    val_output_dir = (\n",
    "        cfg.prediction_output / configuration / \"val\"\n",
    "        if cfg.prediction_output\n",
    "        else fold_dir / \"pipeline_val_predictions\"\n",
    "    )\n",
    "    run_inference(\n",
    "        model_dir=model_dir,\n",
    "        fold=cfg.fold,\n",
    "        inputs=val_inputs,\n",
    "        output_dir=val_output_dir,\n",
    "        device=cfg.device,\n",
    "        checkpoint_name=cfg.checkpoint_name,\n",
    "        save_probabilities=cfg.save_probabilities,\n",
    "        overwrite=cfg.overwrite,\n",
    "        num_preprocess_workers=cfg.inference_preprocess_workers,\n",
    "        num_export_workers=cfg.inference_export_workers,\n",
    "    )\n",
    "    summary_file = val_output_dir / \"summary.json\"\n",
    "    summary = compute_validation_metrics(\n",
    "        predictions_dir=val_output_dir,\n",
    "        dataset_dir=dataset_dir,\n",
    "        plans_identifier=plans_identifier,\n",
    "        output_filename=summary_file,\n",
    "    )\n",
    "    results[(configuration, \"validation\")] = summary\n",
    "\n",
    "if not cfg.skip_test_inference and case_splits.get(\"test\"):\n",
    "    test_inputs = build_inference_input_lists(dataset_dir / \"imagesTs\", case_splits[\"test\"], file_ending)\n",
    "    test_output_dir = (\n",
    "        cfg.prediction_output / configuration / \"test\"\n",
    "        if cfg.prediction_output\n",
    "        else fold_dir / \"pipeline_test_predictions\"\n",
    "    )\n",
    "    run_inference(\n",
    "        model_dir=model_dir,\n",
    "        fold=cfg.fold,\n",
    "        inputs=test_inputs,\n",
    "        output_dir=test_output_dir,\n",
    "        device=cfg.device,\n",
    "        checkpoint_name=cfg.checkpoint_name,\n",
    "        save_probabilities=cfg.save_probabilities,\n",
    "        overwrite=cfg.overwrite,\n",
    "        num_preprocess_workers=cfg.inference_preprocess_workers,\n",
    "        num_export_workers=cfg.inference_export_workers,\n",
    "    )\n",
    "    if cfg.export_test_pngs:\n",
    "        png_root = cfg.png_output_root or (cfg.data_root / \"test_labels\")\n",
    "        png_root = Path(png_root)\n",
    "        if len(active_configurations) > 1 and cfg.png_output_root is None:\n",
    "            png_root = png_root / configuration\n",
    "        export_predictions_to_png(test_output_dir, png_root, case_folder_map)\n",
    "        if cfg.log_to_stdout:\n",
    "            print(f\"Test PNG segmentations saved to {png_root}\")\n",
    "    if cfg.bounding_box_prompts:\n",
    "        bbox_target = test_output_dir / \"test_bboxes.json\"\n",
    "        if not bbox_target.exists():\n",
    "            parse_bbox_prompts(cfg.bounding_box_prompts, bbox_target)\n",
    "    results[(configuration, \"test\")] = str(test_output_dir)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec98c1-849b-4e6f-92e5-787927194c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
