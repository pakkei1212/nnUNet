{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7cb5e05e",
      "metadata": {},
      "source": [
        "# nnU-Net Pipeline Walkthrough"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e18a1b8a",
      "metadata": {},
      "source": [
        "This notebook replicates the end-to-end nnU-Net pipeline (data preparation, training, inference) in three separate stages.\n",
        "Update the configuration cell, then execute the subsequent sections sequentially."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af11878",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e5c739",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "from skimage import io"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfe433ef",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "Edit the fields of `PipelineConfig` or override `cfg` attributes after instantiation to match your setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8815e6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class PipelineConfig:\n",
        "    data_root: Path = field(default_factory=lambda: Path(\"public_leaderboard_data\"))\n",
        "    dataset_id: int = 500\n",
        "    dataset_name: str = \"AbdominalCTMultiOrgan\"\n",
        "    nnunet_raw: Path = field(default_factory=lambda: Path(\"./nnUNet_raw\"))\n",
        "    nnunet_preprocessed: Path = field(default_factory=lambda: Path(\"./nnUNet_preprocessed\"))\n",
        "    nnunet_results: Path = field(default_factory=lambda: Path(\"./nnUNet_results\"))\n",
        "    configurations: Sequence[str] = (\"3d_fullres\",)\n",
        "    trainer_class: str = \"nnUNetTrainer\"\n",
        "    plans_identifier: str = \"nnUNetPlans\"\n",
        "    fold: str = \"0\"\n",
        "    device: str = \"cuda\"\n",
        "    num_gpus: int = 1\n",
        "    num_processes_fingerprint: int = 8\n",
        "    num_processes_preprocess: int = 8\n",
        "    prediction_output: Optional[Path] = None\n",
        "    checkpoint_name: str = \"checkpoint_final.pth\"\n",
        "    planner_class: str = \"nnUNetPlannerResEncM\"\n",
        "    gpu_memory_target: Optional[float] = None\n",
        "    preprocessor_class: str = \"DefaultPreprocessor\"\n",
        "    verify_dataset: bool = False\n",
        "    skip_conversion: bool = False\n",
        "    skip_preprocessing: bool = False\n",
        "    skip_training: bool = False\n",
        "    skip_validation_inference: bool = False\n",
        "    skip_test_inference: bool = False\n",
        "    export_test_pngs: bool = True\n",
        "    png_output_root: Optional[Path] = None\n",
        "    overwrite: bool = False\n",
        "    save_probabilities: bool = False\n",
        "    export_validation_probabilities: bool = False\n",
        "    bounding_box_prompts: Optional[Path] = None\n",
        "    only_configuration: Optional[str] = None\n",
        "    log_to_stdout: bool = True\n",
        "\n",
        "    def clone(self) -> \"PipelineConfig\":\n",
        "        return PipelineConfig(**self.__dict__)\n",
        "\n",
        "\n",
        "cfg = PipelineConfig()\n",
        "\n",
        "if cfg.prediction_output is None:\n",
        "    cfg.prediction_output = Path(\"./notebook_predictions\")\n",
        "else:\n",
        "    cfg.prediction_output = Path(cfg.prediction_output)\n",
        "cfg.prediction_output.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04cf5e1",
      "metadata": {},
      "source": [
        "## Utility Functions & Dataset Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb246063",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from nnunetv2.dataset_conversion.generate_dataset_json import generate_dataset_json\n",
        "\n",
        "def configure_environment(cfg: PipelineConfig) -> None:\n",
        "    os.environ.setdefault(\"nnUNet_raw\", str(cfg.nnunet_raw.resolve()))\n",
        "    os.environ.setdefault(\"nnUNet_preprocessed\", str(cfg.nnunet_preprocessed.resolve()))\n",
        "    os.environ.setdefault(\"nnUNet_results\", str(cfg.nnunet_results.resolve()))\n",
        "    for root in (cfg.nnunet_raw, cfg.nnunet_preprocessed, cfg.nnunet_results):\n",
        "        Path(root).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def ensure_dependencies() -> None:\n",
        "    try:\n",
        "        import torch  # noqa: F401\n",
        "    except ImportError as exc:\n",
        "        raise RuntimeError(\"PyTorch is required to run the pipeline. Install project dependencies first.\") from exc\n",
        "\n",
        "\n",
        "def parse_spacing_map(spacing_file: Path) -> Dict[str, Tuple[float, float, float]]:\n",
        "    if not spacing_file.exists():\n",
        "        raise FileNotFoundError(f\"Spacing file not found: {spacing_file}\")\n",
        "    mapping: Dict[str, Tuple[float, float, float]] = {}\n",
        "    with spacing_file.open(\"r\") as f:\n",
        "        for raw_line in f:\n",
        "            line = raw_line.strip()\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "            if \":\" not in line:\n",
        "                continue\n",
        "            key, value = line.split(\":\", 1)\n",
        "            case_id = key.strip().zfill(2)\n",
        "            spacing = eval(value.strip(), {\"__builtins__\": {}})\n",
        "            if not isinstance(spacing, (list, tuple)) or len(spacing) != 3:\n",
        "                raise ValueError(f\"Unexpected spacing entry for case {case_id}: {value}\")\n",
        "            mapping[case_id] = tuple(float(v) for v in spacing)\n",
        "    return mapping\n",
        "\n",
        "\n",
        "def sorted_slice_paths(case_folder: Path) -> List[Path]:\n",
        "    slices = sorted(case_folder.glob(\"*.png\"), key=lambda p: int(p.stem))\n",
        "    if not slices:\n",
        "        raise FileNotFoundError(f\"No PNG slices found in {case_folder}\")\n",
        "    return slices\n",
        "\n",
        "\n",
        "def load_stack(slice_paths: Sequence[Path]) -> np.ndarray:\n",
        "    stack = [io.imread(str(p)) for p in slice_paths]\n",
        "    return np.stack(stack, axis=0)\n",
        "\n",
        "\n",
        "def write_nifti(volume: np.ndarray, spacing: Tuple[float, float, float], output_path: Path, dtype: np.dtype) -> None:\n",
        "    img = sitk.GetImageFromArray(volume.astype(dtype, copy=False))\n",
        "    img.SetSpacing(tuple(float(v) for v in spacing))\n",
        "    img.SetOrigin((0.0, 0.0, 0.0))\n",
        "    img.SetDirection((1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0))\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    sitk.WriteImage(img, str(output_path))\n",
        "\n",
        "\n",
        "def convert_split_to_nnunet(\n",
        "    case_ids: Iterable[str],\n",
        "    image_root: Path,\n",
        "    label_root: Optional[Path],\n",
        "    output_images: Path,\n",
        "    output_labels: Optional[Path],\n",
        "    spacing_map: Dict[str, Tuple[float, float, float]],\n",
        "    prefix: str,\n",
        "    overwrite: bool,\n",
        ") -> Tuple[List[str], Dict[str, str]]:\n",
        "    case_identifiers: List[str] = []\n",
        "    case_mapping: Dict[str, str] = {}\n",
        "    for case_id in sorted(case_ids, key=lambda x: int(x)):\n",
        "        image_case_dir = image_root / case_id\n",
        "        label_case_dir = label_root / case_id if label_root is not None else None\n",
        "        if not image_case_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Missing image folder for case {case_id}: {image_case_dir}\")\n",
        "        spacing = spacing_map.get(case_id)\n",
        "        if spacing is None:\n",
        "            raise KeyError(f\"No spacing metadata for case {case_id} in spacing file\")\n",
        "        case_name = f\"{prefix}_{case_id.zfill(3)}\"\n",
        "        image_output_path = output_images / f\"{case_name}_0000.nii.gz\"\n",
        "        if image_output_path.exists() and not overwrite:\n",
        "            case_identifiers.append(case_name)\n",
        "            case_mapping[case_name] = str(case_id).zfill(2)\n",
        "            continue\n",
        "        slices = sorted_slice_paths(image_case_dir)\n",
        "        volume = load_stack(slices).astype(np.int16, copy=False)\n",
        "        write_nifti(volume, spacing, image_output_path, np.int16)\n",
        "        if label_case_dir is not None:\n",
        "            if output_labels is None:\n",
        "                raise ValueError(\"Label root provided but output label directory missing.\")\n",
        "            label_slices = sorted_slice_paths(label_case_dir)\n",
        "            if len(label_slices) != len(slices):\n",
        "                raise ValueError(\n",
        "                    f\"Mismatched slice count for case {case_id}: {len(slices)} images vs {len(label_slices)} labels\"\n",
        "                )\n",
        "            label_volume = load_stack(label_slices).astype(np.uint8, copy=False)\n",
        "            label_output_path = output_labels / f\"{case_name}.nii.gz\"\n",
        "            write_nifti(label_volume, spacing, label_output_path, np.uint8)\n",
        "        case_identifiers.append(case_name)\n",
        "        case_mapping[case_name] = str(case_id).zfill(2)\n",
        "    return case_identifiers, case_mapping\n",
        "\n",
        "\n",
        "def parse_bbox_prompts(bbox_file: Optional[Path], output_json: Path) -> None:\n",
        "    if not bbox_file or not bbox_file.exists():\n",
        "        return\n",
        "    prompts: Dict[str, Dict[str, Dict[str, Sequence[int]]]] = {}\n",
        "    with bbox_file.open(\"r\") as f:\n",
        "        for raw_line in f:\n",
        "            line = raw_line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            if not line.startswith(\"<\") or \">:\" not in line:\n",
        "                continue\n",
        "            key, value = line.split(\">:\")\n",
        "            triplet = key.strip(\"<>\").split(\",\")\n",
        "            if len(triplet) != 3:\n",
        "                continue\n",
        "            case_id = triplet[0].strip().zfill(2)\n",
        "            slice_idx = triplet[1].strip()\n",
        "            organ_idx = triplet[2].strip()\n",
        "            coords = eval(value.strip(), {\"__builtins__\": {}})\n",
        "            prompts.setdefault(case_id, {}).setdefault(slice_idx, {})[organ_idx] = coords\n",
        "    output_json.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with output_json.open(\"w\") as f:\n",
        "        json.dump(prompts, f, indent=2)\n",
        "\n",
        "\n",
        "def generate_dataset_json_file(\n",
        "    dataset_dir: Path,\n",
        "    num_training_cases: int,\n",
        "    labels: Dict[str, int],\n",
        "    dataset_name: str,\n",
        "    metadata: Dict[str, object],\n",
        ") -> None:\n",
        "    generate_dataset_json(\n",
        "        str(dataset_dir),\n",
        "        channel_names={0: \"CT\"},\n",
        "        labels=labels,\n",
        "        num_training_cases=num_training_cases,\n",
        "        file_ending=\".nii.gz\",\n",
        "        dataset_name=dataset_name,\n",
        "        **metadata,\n",
        "    )\n",
        "\n",
        "\n",
        "def prepare_raw_dataset(cfg: PipelineConfig, dataset_dir: Path) -> Tuple[Dict[str, List[str]], Dict[str, str]]:\n",
        "    spacing_map = parse_spacing_map(cfg.data_root / \"spacing_mm.txt\")\n",
        "    train_ids = [p.name for p in (cfg.data_root / \"train_images\").iterdir() if p.is_dir()]\n",
        "    val_ids = [p.name for p in (cfg.data_root / \"val_images\").iterdir() if p.is_dir()]\n",
        "    test_ids = [p.name for p in (cfg.data_root / \"test1_images\").iterdir() if p.is_dir()]\n",
        "\n",
        "    images_tr = dataset_dir / \"imagesTr\"\n",
        "    labels_tr = dataset_dir / \"labelsTr\"\n",
        "    images_ts = dataset_dir / \"imagesTs\"\n",
        "    images_tr.mkdir(parents=True, exist_ok=True)\n",
        "    labels_tr.mkdir(parents=True, exist_ok=True)\n",
        "    images_ts.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_cases, train_map = convert_split_to_nnunet(\n",
        "        train_ids,\n",
        "        cfg.data_root / \"train_images\",\n",
        "        cfg.data_root / \"train_labels\",\n",
        "        images_tr,\n",
        "        labels_tr,\n",
        "        spacing_map,\n",
        "        prefix=\"ct\",\n",
        "        overwrite=cfg.overwrite,\n",
        "    )\n",
        "    val_cases, val_map = convert_split_to_nnunet(\n",
        "        val_ids,\n",
        "        cfg.data_root / \"val_images\",\n",
        "        cfg.data_root / \"val_labels\",\n",
        "        images_tr,\n",
        "        labels_tr,\n",
        "        spacing_map,\n",
        "        prefix=\"ct\",\n",
        "        overwrite=cfg.overwrite,\n",
        "    )\n",
        "    test_cases, test_map = convert_split_to_nnunet(\n",
        "        test_ids,\n",
        "        cfg.data_root / \"test1_images\",\n",
        "        None,\n",
        "        images_ts,\n",
        "        None,\n",
        "        spacing_map,\n",
        "        prefix=\"ct\",\n",
        "        overwrite=cfg.overwrite,\n",
        "    )\n",
        "\n",
        "    metadata = {\n",
        "        \"training_cases\": train_cases,\n",
        "        \"validation_cases\": val_cases,\n",
        "        \"test_cases\": test_cases,\n",
        "        \"spacing_file\": str((cfg.data_root / \"spacing_mm.txt\").resolve()),\n",
        "        \"case_folder_map\": {**train_map, **val_map, **test_map},\n",
        "    }\n",
        "\n",
        "    labels = {\"background\": 0}\n",
        "    for organ_idx in range(1, 13):\n",
        "        labels[f\"organ_{organ_idx:02d}\"] = organ_idx\n",
        "\n",
        "    generate_dataset_json_file(\n",
        "        dataset_dir=dataset_dir,\n",
        "        num_training_cases=len(train_cases) + len(val_cases),\n",
        "        labels=labels,\n",
        "        dataset_name=dataset_dir.name,\n",
        "        metadata=metadata,\n",
        "    )\n",
        "\n",
        "    splits = [{\"train\": train_cases, \"val\": val_cases}]\n",
        "    splits_file = dataset_dir / \"splits_final.json\"\n",
        "    if not splits_file.exists() or cfg.overwrite:\n",
        "        with splits_file.open(\"w\") as f:\n",
        "            json.dump(splits, f, indent=2)\n",
        "\n",
        "    if cfg.bounding_box_prompts:\n",
        "        parse_bbox_prompts(cfg.bounding_box_prompts, dataset_dir / \"test_bboxes.json\")\n",
        "\n",
        "    return {\"train\": train_cases, \"val\": val_cases, \"test\": test_cases}, {**train_map, **val_map, **test_map}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8fc9abe",
      "metadata": {},
      "source": [
        "## Training & Inference Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0d6f5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def run_planning_and_preprocessing(cfg: PipelineConfig, configurations: Sequence[str]) -> str:\n",
        "    from nnunetv2.experiment_planning.plan_and_preprocess_api import (\n",
        "        extract_fingerprints,\n",
        "        plan_experiments,\n",
        "        preprocess,\n",
        "    )\n",
        "\n",
        "    dataset_ids = [cfg.dataset_id]\n",
        "    extract_fingerprints(\n",
        "        dataset_ids,\n",
        "        num_processes=cfg.num_processes_fingerprint,\n",
        "        check_dataset_integrity=cfg.verify_dataset,\n",
        "        clean=True,\n",
        "        verbose=True,\n",
        "    )\n",
        "    resulting = plan_experiments(\n",
        "        dataset_ids,\n",
        "        experiment_planner_class_name=cfg.planner_class,\n",
        "        preprocess_class_name=cfg.preprocessor_class,\n",
        "        gpu_memory_target_in_gb=cfg.gpu_memory_target,\n",
        "    )\n",
        "    preprocess(\n",
        "        dataset_ids,\n",
        "        plans_identifier=resulting or cfg.plans_identifier,\n",
        "        configurations=tuple(configurations),\n",
        "        num_processes=tuple([cfg.num_processes_preprocess] * len(configurations)),\n",
        "        verbose=True,\n",
        "    )\n",
        "    return resulting or cfg.plans_identifier\n",
        "\n",
        "\n",
        "def build_model_output_dir(dataset_name: str, trainer_class: str, plans_identifier: str, configuration: str) -> Path:\n",
        "    base = Path(os.environ[\"nnUNet_results\"])\n",
        "    return base / dataset_name / f\"{trainer_class}__{plans_identifier}__{configuration}\"\n",
        "\n",
        "\n",
        "def run_training_stage(\n",
        "    cfg: PipelineConfig,\n",
        "    dataset_name: str,\n",
        "    configuration: str,\n",
        "    plans_identifier: str,\n",
        ") -> None:\n",
        "    import torch\n",
        "    from nnunetv2.run.run_training import run_training\n",
        "\n",
        "    torch_device = torch.device(cfg.device)\n",
        "    run_training(\n",
        "        dataset_name,\n",
        "        configuration=configuration,\n",
        "        fold=cfg.fold,\n",
        "        trainer_class_name=cfg.trainer_class,\n",
        "        plans_identifier=plans_identifier,\n",
        "        num_gpus=cfg.num_gpus,\n",
        "        device=torch_device,\n",
        "        export_validation_probabilities=cfg.export_validation_probabilities,\n",
        "    )\n",
        "\n",
        "\n",
        "def run_inference(\n",
        "    model_dir: Path,\n",
        "    fold: str,\n",
        "    inputs: Sequence[List[str]],\n",
        "    output_dir: Path,\n",
        "    device: str,\n",
        "    checkpoint_name: str,\n",
        "    save_probabilities: bool,\n",
        "    overwrite: bool,\n",
        ") -> None:\n",
        "    import torch\n",
        "    from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
        "\n",
        "    predictor = nnUNetPredictor(device=torch.device(device))\n",
        "    predictor.initialize_from_trained_model_folder(str(model_dir), use_folds=(fold,), checkpoint_name=checkpoint_name)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    predictor.predict_from_files(\n",
        "        list(inputs),\n",
        "        str(output_dir),\n",
        "        save_probabilities=save_probabilities,\n",
        "        overwrite=overwrite,\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_validation_metrics(\n",
        "    predictions_dir: Path,\n",
        "    dataset_dir: Path,\n",
        "    plans_identifier: str,\n",
        "    output_filename: Optional[Path] = None,\n",
        ") -> Dict[str, object]:\n",
        "    from nnunetv2.evaluation.evaluate_predictions import compute_metrics_on_folder2\n",
        "\n",
        "    dataset_json = dataset_dir / \"dataset.json\"\n",
        "    plans_file = Path(os.environ[\"nnUNet_preprocessed\"]) / dataset_dir.name / f\"{plans_identifier}.json\"\n",
        "    gt_folder = dataset_dir / \"labelsTr\"\n",
        "    return compute_metrics_on_folder2(\n",
        "        str(gt_folder),\n",
        "        str(predictions_dir),\n",
        "        str(dataset_json),\n",
        "        str(plans_file),\n",
        "        output_file=str(output_filename) if output_filename else None,\n",
        "    )\n",
        "\n",
        "\n",
        "def build_inference_input_lists(image_dir: Path, case_ids: Sequence[str], file_ending: str) -> List[List[str]]:\n",
        "    inputs: List[List[str]] = []\n",
        "    for case in case_ids:\n",
        "        image_file = image_dir / f\"{case}_0000{file_ending}\"\n",
        "        if not image_file.exists():\n",
        "            raise FileNotFoundError(f\"Missing input volume for inference: {image_file}\")\n",
        "        inputs.append([str(image_file)])\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def convert_nifti_to_png_slices(nifti_file: Path, output_dir: Path) -> None:\n",
        "    img = sitk.ReadImage(str(nifti_file))\n",
        "    data = sitk.GetArrayFromImage(img).astype(np.uint8, copy=False)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for idx, slice_arr in enumerate(data, start=1):\n",
        "        slice_path = output_dir / f\"{idx}.png\"\n",
        "        io.imsave(str(slice_path), slice_arr, check_contrast=False)\n",
        "\n",
        "\n",
        "def export_predictions_to_png(\n",
        "    predictions_dir: Path,\n",
        "    output_root: Path,\n",
        "    case_folder_map: Dict[str, str],\n",
        ") -> None:\n",
        "    prediction_files = sorted(predictions_dir.glob(\"*.nii.gz\"))\n",
        "    if not prediction_files:\n",
        "        return\n",
        "    output_root.mkdir(parents=True, exist_ok=True)\n",
        "    for prediction_file in prediction_files:\n",
        "        filename = prediction_file.name\n",
        "        identifier = filename[:-7] if filename.endswith(\".nii.gz\") else prediction_file.stem\n",
        "        folder_name = case_folder_map.get(identifier, identifier.split(\"_\")[-1])\n",
        "        folder_name = str(int(folder_name)).zfill(2) if folder_name.isdigit() else folder_name\n",
        "        convert_nifti_to_png_slices(prediction_file, output_root / folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7f2f9cd",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "Run this cell to convert the PNG slices into the nnU-Net raw data structure (unless skipped)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95e42cbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "configure_environment(cfg)\n",
        "ensure_dependencies()\n",
        "\n",
        "dataset_name = f\"Dataset{cfg.dataset_id:03d}_{cfg.dataset_name}\"\n",
        "dataset_dir = Path(os.environ[\"nnUNet_raw\"]) / dataset_name\n",
        "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if cfg.skip_conversion and (dataset_dir / \"dataset.json\").exists():\n",
        "    with (dataset_dir / \"dataset.json\").open(\"r\") as f:\n",
        "        dataset_meta = json.load(f)\n",
        "    case_splits = {\n",
        "        \"train\": dataset_meta.get(\"training_cases\", []),\n",
        "        \"val\": dataset_meta.get(\"validation_cases\", []),\n",
        "        \"test\": dataset_meta.get(\"test_cases\", []),\n",
        "    }\n",
        "    raw_map = dataset_meta.get(\"case_folder_map\", {}) or {}\n",
        "    case_folder_map = {k: str(v).zfill(2) for k, v in raw_map.items()}\n",
        "    if cfg.log_to_stdout:\n",
        "        print(\"Skipping dataset conversion (dataset.json already present).\")\n",
        "else:\n",
        "    case_splits, case_folder_map = prepare_raw_dataset(cfg, dataset_dir)\n",
        "    if cfg.log_to_stdout:\n",
        "        print(f\"Converted dataset stored at {dataset_dir}\")\n",
        "\n",
        "if not case_folder_map:\n",
        "    case_folder_map = {identifier: identifier.split(\"_\")[-1] for identifier in case_splits.get(\"test\", [])}\n",
        "\n",
        "active_configurations = list((cfg.only_configuration,) if cfg.only_configuration else cfg.configurations)\n",
        "case_splits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81a1f8f0",
      "metadata": {},
      "source": [
        "## Training\n",
        "Preprocess the dataset and train the requested configurations/folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f6f692",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if not cfg.skip_preprocessing:\n",
        "    plans_identifier = run_planning_and_preprocessing(cfg, active_configurations)\n",
        "else:\n",
        "    plans_identifier = cfg.plans_identifier\n",
        "    if cfg.log_to_stdout:\n",
        "        print(\"Skipping planning & preprocessing.\")\n",
        "\n",
        "model_directories: Dict[str, Path] = {}\n",
        "for configuration in active_configurations:\n",
        "    model_dir = build_model_output_dir(dataset_name, cfg.trainer_class, plans_identifier, configuration)\n",
        "    model_directories[configuration] = model_dir\n",
        "    if cfg.skip_training:\n",
        "        if cfg.log_to_stdout:\n",
        "            print(f\"Skipping training for configuration {configuration}.\")\n",
        "        continue\n",
        "    if cfg.log_to_stdout:\n",
        "        print(f\"Starting training for configuration {configuration} (fold {cfg.fold})...\")\n",
        "    run_training_stage(cfg, dataset_name, configuration, plans_identifier)\n",
        "\n",
        "model_directories"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a625fda",
      "metadata": {},
      "source": [
        "## Inference & Evaluation\n",
        "Generate validation metrics and export test predictions (optionally as PNG slices)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e7746a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "results: Dict[Tuple[str, str], object] = {}\n",
        "file_ending = \".nii.gz\"\n",
        "\n",
        "for configuration, model_dir in model_directories.items():\n",
        "    fold_dir = model_dir / f\"fold_{cfg.fold}\"\n",
        "    if not fold_dir.exists():\n",
        "        raise FileNotFoundError(f\"Expected trained fold directory does not exist: {fold_dir}\")\n",
        "\n",
        "    if not cfg.skip_validation_inference and case_splits.get(\"val\"):\n",
        "        val_inputs = build_inference_input_lists(dataset_dir / \"imagesTr\", case_splits[\"val\"], file_ending)\n",
        "        val_output_dir = (\n",
        "            cfg.prediction_output / configuration / \"val\"\n",
        "            if cfg.prediction_output\n",
        "            else fold_dir / \"pipeline_val_predictions\"\n",
        "        )\n",
        "        run_inference(\n",
        "            model_dir=model_dir,\n",
        "            fold=cfg.fold,\n",
        "            inputs=val_inputs,\n",
        "            output_dir=val_output_dir,\n",
        "            device=cfg.device,\n",
        "            checkpoint_name=cfg.checkpoint_name,\n",
        "            save_probabilities=cfg.save_probabilities,\n",
        "            overwrite=cfg.overwrite,\n",
        "        )\n",
        "        summary_file = val_output_dir / \"summary.json\"\n",
        "        summary = compute_validation_metrics(\n",
        "            predictions_dir=val_output_dir,\n",
        "            dataset_dir=dataset_dir,\n",
        "            plans_identifier=plans_identifier,\n",
        "            output_filename=summary_file,\n",
        "        )\n",
        "        results[(configuration, \"validation\")] = summary\n",
        "\n",
        "    if not cfg.skip_test_inference and case_splits.get(\"test\"):\n",
        "        test_inputs = build_inference_input_lists(dataset_dir / \"imagesTs\", case_splits[\"test\"], file_ending)\n",
        "        test_output_dir = (\n",
        "            cfg.prediction_output / configuration / \"test\"\n",
        "            if cfg.prediction_output\n",
        "            else fold_dir / \"pipeline_test_predictions\"\n",
        "        )\n",
        "        run_inference(\n",
        "            model_dir=model_dir,\n",
        "            fold=cfg.fold,\n",
        "            inputs=test_inputs,\n",
        "            output_dir=test_output_dir,\n",
        "            device=cfg.device,\n",
        "            checkpoint_name=cfg.checkpoint_name,\n",
        "            save_probabilities=cfg.save_probabilities,\n",
        "            overwrite=cfg.overwrite,\n",
        "        )\n",
        "        if cfg.export_test_pngs:\n",
        "            png_root = cfg.png_output_root or (cfg.data_root / \"test_labels\")\n",
        "            png_root = Path(png_root)\n",
        "            if len(active_configurations) > 1 and cfg.png_output_root is None:\n",
        "                png_root = png_root / configuration\n",
        "            export_predictions_to_png(test_output_dir, png_root, case_folder_map)\n",
        "            if cfg.log_to_stdout:\n",
        "                print(f\"Test PNG segmentations saved to {png_root}\")\n",
        "        if cfg.bounding_box_prompts:\n",
        "            bbox_target = test_output_dir / \"test_bboxes.json\"\n",
        "            if not bbox_target.exists():\n",
        "                parse_bbox_prompts(cfg.bounding_box_prompts, bbox_target)\n",
        "        results[(configuration, \"test\")] = str(test_output_dir)\n",
        "\n",
        "results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
